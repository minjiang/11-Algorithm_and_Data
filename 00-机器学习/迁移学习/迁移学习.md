# 迁移学习
在imagenet上学习的模型也能用于其他的分类任务（比如医学的肿瘤发现等），但实际问题，往往是小样本数据，迁移学习的发展显得格外有意义。
2017年初，吴恩达接受技术研究和分析公司Gigaom的专访时，说到“迁移学习是未来五年的重要研究方向”，目前国内香港科技大学的杨强教授，在公开演讲中多次介绍迁移学习。

迁移学习广泛存在于人类的活动中，两个不同的领域共享的因素越多，迁移学习就越容易，否则就越困难，甚至出现“负迁移”，产生副作用。


## 更形象地理解
一个人要是学会了自行车，那他就很容易学会开摩托车；一个人要是熟悉五子棋，也可以轻松地将知识迁移到学习围棋中。
但是有时候看起来很相似的事情，却有可能产生“负迁移”，比如，学会自行车的人来学习三轮车反而不适应，因为它们的重心位置不同。

## 迁移学习的报告
问题1：迁移学习问题里面，数据domain不同，是不是标签空间也不同？
回答：目前主要研究的内容都基于相同标签空间。虽然迁移学习可以针对联合概率分布不同的问题，但更多还是研究marginal分布不同的情况，也就是标签空间分布是通常假设一致的，比如每个domain都是针对C类的问题。

问题2：目前有没有关于时序数据的迁移学习方法，比如视频序列的迁移？
回答：现在的研究方法主要集中在静态图像数据。视频序列的迁移还没有见到，但是是一个很有意义的方向。其实，迁移学习是解决分布差异的学习问题，对应用场景并不受限。

问题3：实际研究过程中怎样选择合适的迁移方法呢？
回答：我们一般要针对具体任务和数据特点选择合适的迁移方法。如果针对应用中，要采用深度学习的话，可以构造深度特征迁移模型。

问题4：迁移学习等于预训练吗?
回答：**两者是不等价的。一般我们认为预训练是迁移学习的一种方式**。实际上，迁移学习的理论与方法比预训练要宽泛的多，有自己的理论基础。并且，预训练一般都是指在深度学习的框架之下，选择一种较好的初始参数，然后通过调优的方式提高target数据集上的分类精度。迁移学习显然还包括很多具体的浅层学习模型，比如我们常见的MMD及其变体。

问题5：关于zero-shot learning最新的研究方向大概有哪些?
回答：零样本学习ZSL是目前比较前沿的弱监督学习方法，主要研究思路还是考虑semantic embedding，通过构建可见类样本的特征，标签和语义嵌入之间的关系，实现对不可见类样本的标签预测。

问题6：理论上MMD要求的函数是非线性的，那么理论上线性函数不能满足要求？
回答：MMD要求的函数F实际上需要满足两个条件，才能满足MMD的两个性质。当且仅当两个分布p和q相等时，MMD=0，要求F必须足够rich。为了使MMD的经验估计能够随着样本规模的增大迅速收到到它的期望，要求F足够restrictive。作者证明当F是universal RKHS上的单位球时，可以满足这两个要求。

问题7：做多模态问题时，有没有必要让多个数据源的表达特征尽可能近？
回答：迁移学习可以用于解决跨域，跨媒体和跨模态的学习问题。再进行多模态研究时，一般从两个角度，第一种就是学习度量，使得两个模态的数据信息更加相似，学习出两个模态共同的特征。比如可见光与近红外人脸识别。另一种，就是结合多模态的特征信息，将其modality-specific的特征进行级联，提升识别效果。

问题9：迁移学习可以用于Image Caption吗？
回答：迁移学习是可以用于image caption的。Image caption相当于是学习图像特征到文本的一种映射，如果遇到图像分布差异时，这种映射可能会性能下降。因此，也需要利用迁移学习首先完成特征分布的对齐，这样会提升cross-domain下的image caption性能。

问题10：Stargan这种，对于不同数据集上的不同领域的迁移学习，有没有什么见解？是否了解其他的相关工作？
回答：我们组也在做cycleGAN在迁移学习中的应用，这里的迁移主要是目标分布下的图像合成，在跨姿态人脸识别方面以及Re-ID方面都有很多的工作，通过生成与目标分布一致的图像，并对合成的图像进行特征表示和识别。这个方向比较热。

# 参考资料
valse的某一次报告